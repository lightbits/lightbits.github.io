<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<title>v4l2 adventures: Real-time capture for computer vision</title>
<link rel="stylesheet" href="../style.css" type="text/css">
<script src="../analytics.js"></script>
</head>
<body>

<h1 id="v4l2-adventures-real-time-video-for-computer-vision">v4l2 adventures: Real-time video for computer vision</h1>
<p>For the past few weeks I have been working with camera capture on Linux for a computer vision project: tracking Roomba robot vacuum cleaners from a drone. This is in preparation for <a href="ascendntnu.no">my university team</a>&#39;s entry in the annual <a href="http://www.aerialroboticscompetition.org/">International Aerial Robotics Competition (IARC)</a> for autonomous drones. In the current challenge, which has been unsolved since 2014, participants must build a drone that can navigate a 20m x 20m indoor space by itself, without GPS, and herd ten Roombas from one side to the other through physical interaction. Sounds interesting? follow our blog at <a href="ascendntnu.no">Ascend NTNU</a>!</p>
<p><img src="venue1.jpg" alt=""></p>
<p>To stay afloat long enough and abide with the rules regulating size, the drones entering this competition tend to use a &quot;low-power&quot; on-board computer, such as the Odroid, the Intel NUC, or lately the Nvidia Jetson chips. These are affordable, embedded devices that can fit comfortably on a small to medium sized drone, while still being decent enough to do on-board image processing. Although these things are computational beasts compared to what we had ten or twenty years ago, they are too weak to be carelessly programmed; the speed of your processing algorithm puts a hard limit on the physical maneuvers and speeds your drone can attain.</p>
<p>For example, our team uses a 180 degree, 60 FPS fisheye-lens camera to detect and track Roombas seen below the drone. The algorithm that does the visual detection and tracking must run fast enough to provide input for the control loop in charge of following and landing on the Roombas (moving at a whopping 0.33 meters per second).</p>
<p>Making the algorithm fast is both a matter of doing as little work as possible, but also minimizing time spent getting images off the camera and into an appropriate format: the latter of which will be the focus of this post. I may do a follow-up describing the algorithms themselves, but for now I will describe some tips and tricks I learned that can be useful to the eager roboticist doing similar real-time camera processing work.</p>
<h2 id="camera-libraries">Camera libraries</h2>
<p>Before we get to the meat of this article - doing camera capture quickly - let&#39;s look at how to do camera capture at all. Aside from writing a driver that communicates with the camera manually, we have several libraries that make life easier, the most popular ones being: ffmpeg, gstreamer and v4l2.</p>
<p><strong>ffmpeg and gstreamer</strong> can be considered more high-level of the three, in the sense that they provide a wrapper around v4l2 (on linux), but also by offering streaming across applications on the same machine, over a local network, or even over the internet.</p>
<p>Streaming can be of great value early in development, especially so if you have multiple people on your team who use data from the camera. In such cases, it can be a productivity boost, even if only streaming across applications on the same device, because your team can develop and test their stuff independently first, before worrying about how to optimally share the camera data between your programs. You can also postpone running programs on the drone itself, even if the camera has been attached to it, by making it stream over local network onto your computer (or multiple computers at the same time).</p>
<p><strong>v4l2 sticks out</strong> because it can offer better control and more transparency of what is going on and where milliseconds are being spent: you decide how data is moved around in memory, you request the capture output format, and you do the decoding. This is useful if the above two libraries do not offer the level of control you want (i.e. device-specific stuff like H264 decompression in hardware), and if you have a need to know precisely how long it takes from a frame is captured until it is delivered to your application.</p>
<p>There are a couple of introductions to v4l2 online: the official documentation is of great value, and served as background for the tips below, but can be dry if you&#39;re just getting started. There&#39;s some simple example code that does the bare minimum to get camera capture working, found on various blogs and github repositories. I&#39;ll also add my <a href="github.com/lightbits/usbcam">usbcam.h</a> library to that list (a single header file in the style of Sean Barrett), which hopefully serves as a fully functioning library, but also as a base that you can modify to your liking.</p>
<p>Now then, if you decide that you need the flexibility provided by v4l2, you&#39;ll want to keep reading for some tips that can be useful for real-time computer vision:</p>
<h2 id="tip-1-manage-your-buffers-">Tip 1: Manage your buffers!</h2>
<p>In v4l2 there are several modes by which you can read frames from the camera, but the one we will look at is the mmap method. In the mmap method, you and the camera share a set of buffers that the camera will write to and you will read from. By sharing them, using mmap (memory mapping), you avoid potentially a expensive memory copy  from the camera to your application.</p>
<p>But, in setting up the mmap method, you have a choice of how many buffers to request. Initially, I didn&#39;t know why I might care about that, and instead of reading the manual, I just left it at whatever number was in the example code I was shamelessly copying. It would be a whole year later before I realized the purpose of these buffers, and also that the example code was not suited for our real-time computer vision application.</p>
<p>We had noticed odd behaviour from time to time, when doing various prototypes, but nothing severe during live flights or logging. Recently though, I ran into a problem on the Jetson, where I was simply unable to get the maximum framerate, no matter how I controlled the exposure. After finally consulting the v4l2 manual, I discovered that the problem had to do with how the example code was managing its buffers.</p>
<p>To sum up, here is how buffers work:</p>
<ul>
<li>The camera needs buffers to do its writing.</li>
<li>You give it a buffer for writing by queuing it.</li>
<li>You get a buffer for reading by dequeuing it.</li>
<li>When you dequeue a buffer the camera can no longer write to it.</li>
<li>The camera does not overwrite buffers: a buffer must be dequeued and queued again to be available for writing.</li>
</ul>
<p>In the example code, the code would request a number of buffers (3 by default), queue them all, and tell the camera to start recording. It would then enter a loop where it waits for a new frame, dequeues it, and makes it available for processing (i.e. computer vision or logging). After processing, the buffer would be requeued.</p>
<p>Given the rules above, the cause for our problems becomes evident: if the processing time per frame is too long, the camera runs out of buffers to write to. This leads to two effects: one being that subsequent frames are dropped, because the camera is not allowed to overwrite buffers already written to; they must be dequeued and requeued first.</p>
<p>The second effect is that in the next loop iteration, the buffer that is dequeued is not necessarily the latest one. In fact, on all the cameras I tested with, the order of dequeuing went from oldest to newest: i.e. if you queue buffers a, b, c, in that order, then the camera will write to them in the same order, and subsequent dequeues will give a, b, c.</p>
<p>This was not what we wanted: in our case, we don&#39;t care about old frames, we just want the latest information. For example, if you have an object detection algorithm that works on stand-alone frames, you will only care about the most recent one, because it gives you the most recent position of the object, and also because your algorithm does not keep track of state across frames. (On the other hand, if your algorithm does some sort of tracking across frames, you might want the deltatime between frames to be consistent: i.e. by never skipping frames).</p>
<p>The solution to these problems was simple: to get the most recent frame, make sure to dequeue <em>all</em> buffers and choose the most recent one. Depending on the camera driver, this can be done by dequeuing buffers until there are no more available, which can be checked using linux file descriptors and polling. If you, for some reason, don&#39;t get buffers in a chronological order, you might need to compare timestamps as well.</p>
<p>Then, after picking the most recent buffer, queue <em>all</em> the buffers again. In this sense, the number of queued buffers represents how much leeway your algorithm has in its processing time. You want to make sure you queue enough buffers for the camera to use while you are busy processing.</p>
<p>Let&#39;s look at an example with three buffers. Right after starting capture, we have queued all the buffers. The first buffer we dequeue is buffer 1. We then take a long time processing this frame:</p>
<p><img src="buffers1.png" alt=""></p>
<p>In fact, we spent so much time, that the camera wrote to buffer 2 and 3 while we were busy. When it was time to write the fourth frame, the camera has no more buffers to use, and, since it will not overwrite previous buffers, the frame is dropped.</p>
<p><img src="buffers2.png" alt=""></p>
<p>The next time we dequeue a buffer, we then get buffer 2; which is not the latest buffer, nor the latest frame we could have gotten had we allocated more buffers. When dequeuing, we also requeue the buffer we just finished processing. But the camera will not write into this until the next frame again.</p>
<p>In summary, to ensure that we get the latest frame possible, we need to:</p>
<ul>
<li>Allocate as many buffers as we predict can get filled during our processing time</li>
<li>Dequeue all the buffers before we start processing, pick the latest one, and requeue all the remaining</li>
</ul>
<h2 id="tip-2-use-turbojpeg-for-jpeg-decompression">Tip 2: Use turbojpeg for JPEG decompression</h2>
<p>Some cameras output (todo: links?) in a somewhat raw format that can be converted to RGB quickly: i.e. the Bayer format, where each pixel contains information for one color channel only, and the other two channels are reconstructed from its neighbours. Another format is YUV, where the bits per pixel is reduced by taking advantage of properties of human perception and operating in a different color space: i.e. YUV 4:2:2 uses four bits for brightness, and two bits for two color components (a technique known as <a href="https://en.wikipedia.org/wiki/Chroma_subsampling">chroma subsampling</a>), for a total of 8 bits per pixel.</p>
<p>Unfortunately, some cameras only give you the option of JPEG output. In this case, you want to decompress it as fast as possible. In my search I found two libraries that are particularly interesting: stb_image, and turbojpeg.</p>
<p>stb_image is super easy to use on any OS and is great for prototyping. But, since it&#39;s not as fast as turbojpeg, I would not use it for real-time decompression. Unfortunately, using turbojpeg is more involved than simply downloading a file from github and dropping it into your source directory. But the speed boost you get is worth the hassle.</p>
<p>I had a hard time finding out how to actually use turbojpeg: all google gave me were some example code from StackOverflow, and searching for the function names did not lead me to their documentation page. But, for prosperity, here&#39;s a <a href="http://www.libjpeg-turbo.org/Documentation/Documentation">link to it</a>. You can find different versions under the TurboJPEG C API header. If you want a code example, you can either look at some of the Stackoverflow posts, or you can check out my <a href="github.com/lightbits/usbcam">usbcam.h</a> repository, which contains a small snippet for doing JPEG to RGB decompression.</p>
<p>Once you&#39;ve got that going, below are some tips that can improve the decompression speed even further:</p>
<h2 id="tip-3-make-turbojpeg-do-downsampling-during-decompression">Tip 3: Make turbojpeg do downsampling during decompression</h2>
<p>If your computer vision algorithm downsamples the image before processing it --- such as in a neural network, or in the Roomba detector above --- you can specify the desired resolution to turbojpeg, which will include it while decompressing. Compared to decompressing in full resolution and downsampling afterwards, this significantly reduces decompression time and overall preprocessing time. Plus, there&#39;s less code, since you don&#39;t need the downsampling!</p>
<p>As a general computer vision tip, you should also ask yourself if you <em>can</em> downsample the image, if you&#39;re not already. Do you really need to run that neural network at 1080p? Maybe the results are still acceptable when downsampling? Maybe they are better even due to the lowpass filtering that occurs (unless you just drop pixels while downsampling).</p>
<h2 id="tip-4-do-you-really-need-rgb-">Tip 4: Do you really need RGB?</h2>
<p>Another thing that can speed up decompression is to avoid generating the RGB output altogether. JPEG store the image data (compressed, mind you) in a YUV-like format that, roughly speaking, describes brightness (Y) and hue (UV). If you only need grayscale, you can use the Y value directly. If you need color, maybe UV would be a better space, since it is seperated from brightness.</p>
<p>You can request that turbojpeg perform the JPEG decompression but skip the color conversion step, by using the <code>tjDecompressToYUV</code> function. todo: link documentation.</p>
<p>todo: difference between TJPF_GRAY and Y?</p>

<div class="footer">
<a href="http://lightbits.github.io/">Archive</a>
<a href="https://twitter.com/uint9">Twitter</a>
<a href="https://github.com/lightbits">Github</a>
</div>

</body>
</html>
